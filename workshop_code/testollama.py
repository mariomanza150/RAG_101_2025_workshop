#!/usr/bin/env python3
"""
A refactored script to test the Ollama chat model with RAG (Retrieval Augmented Generation) support.
It loads documents from 'data.json' (under the key 'sample_embeddings') into a Chroma vector store
using embeddings generated by Ollama's embed model ("mxbai-embed-large"). Predefined questions (from
questions.json) and interactive queries can be augmented with context from the vector store when RAG mode
is enabled.
"""

from typing import Any, Dict, List, Optional, Union

import ollama

from project_tools.config import CHAT_MODEL, EMBED_MODEL
from project_tools.utils import (
    get_embedding,
    initialize_model,
    initialize_vector_store,
    load_json_file,
)


def load_questions() -> Union[List[Any], bool]:
    """
    If the file is not found, an attempt is made to decrypt the file using data_vault.tool_run.
    """
    try:
        return load_json_file()["questions"]
    except FileNotFoundError:
        from project_tools.data_vault import tool_run

        if tool_run():
            return load_json_file()["questions"]
    return False


def augment_query_with_context(query: str, vector_collection: Any) -> str:
    """
    Augments the given query with context by retrieving similar documents from the vector store.
    """
    try:
        query_embedding = get_embedding(query)

        results = vector_collection.query(
            query_embeddings=[query_embedding], n_results=1
        )

        retrieved_docs = results.get("documents", [[]])[0]

        if retrieved_docs:
            context = "\n".join(retrieved_docs)
            print("\nRetrieved Context:")
            print(context)
            return {
                "role": "system",
                "content": f"Use the following CONTEXT to answer a users query in a natural way: {context}",
            }
    except Exception as e:
        print("Error retrieving context from vector store:", e)
    return query


def process_question(
    chat_model: str,
    question_input: Union[str, Dict[str, str]],
    use_rag: bool = False,
    vector_collection: Optional[Any] = None,
) -> None:
    """
    Processes a single question:
      - Extracts the question text if provided as a dict.
      - If RAG mode is enabled, augments the query with context from the vector store.
      - Streams the response from the chat model.
    """
    if isinstance(question_input, dict):
        query_text = question_input.get("q", "")
        print("Question:", query_text)
        print("RAG Benefit:", question_input.get("comm", ""))
    else:
        query_text = question_input

    messages = [{"role": "user", "content": f"QUERY: {query_text}"}]
    if use_rag and vector_collection is not None:
        messages.insert(0, augment_query_with_context(query_text, vector_collection))

    if input("\nPress Enter to see the response, s to skip...\n") == "s":
        return

    try:
        response_stream = ollama.chat(
            model=chat_model,
            messages=messages,
            stream=True,
        )
        for chunk in response_stream:
            print(chunk["message"]["content"], end="", flush=True)
    except Exception as e:
        print("Error during chat:", e)
    input("\nPress Enter to continue...")


def main() -> None:
    print("Initializing chat model...")
    initialize_model(CHAT_MODEL)
    print("Chat model ready.\n")

    questions = load_questions()
    use_rag = False
    vector_collection = None

    if questions:
        rag_choice = input("Enable RAG mode? (y/n): ").strip().lower()
        use_rag = rag_choice in ("y", "yes")
        if use_rag:
            print("Initializing embedding model...")
            initialize_model(EMBED_MODEL)
            print("Embedding model ready.\n")
            print("Initializing vector store for RAG...")
            vector_collection = initialize_vector_store()
        print(f"RAG mode {'enabled' if use_rag else 'disabled'}.\n")
        print("Processing predefined test questions...")
        for q in questions:
            print("\n" + "-" * 50)
            process_question(CHAT_MODEL, q, use_rag, vector_collection)

    print("\nEnter your own questions. Type 'exit' or 'quit' to leave.")
    while True:
        print("\n" + "-" * 50)
        user_question = input("\nYour question: ").strip()
        if user_question.lower() in ("exit", "quit", "q"):
            print("Exiting. Goodbye!")
            break
        process_question(CHAT_MODEL, user_question, use_rag, vector_collection)

    if not questions:
        print("Congratulations! You are ready for the workshop!")


if __name__ == "__main__":
    main()
